---
title: "R Notebook"
output: html_notebook
---

```{r}
#### Package
library(readr)
library(tidyverse)
library(gtools)
library(readxl)
library(modeldata)
library(caret)   
library(rsample) 
library(ggplot2)
library(dplyr)
library(gmodels) 
library(class) 
library(C50)
library(rpart)
library(rpart.plot)
```

## Datos

```{r}
## Read
getwd()
rm(list=ls())
df <- read_delim("PARTE 2/bank-additional-full.csv", 
    delim = ";")
## Drop pdays
df <- df %>% select(-pdays)
```
```{r}
## str data
str(df)
```
```{r}
### NAS
colSums(is.na(df))
```

```{r}
## Factor data.
factor_cols<- c('job','marital','education',
              'default','housing','loan',
              'contact','month','day_of_week',
              'poutcome','y')
df[factor_cols] <- lapply(df[factor_cols], factor)
```


## Analisis Exploratorio

### Histogram

```{r}
## Columnas numéricas
numeric_cols <- names(df)[!names(df) %in% factor_cols]
```
```{r}
par(mfrow = c(3, 3), xaxs = "i")
for (i in numeric_cols){
  hist(df[[i]],col='lightblue',main = i,xlab = i)}
```


### Estadisticas descriptiva

```{r}
df %>% select(all_of(numeric_cols)) %>% summary()
```


```{r}
df %>% select(all_of(factor_cols)) %>% summary()
```
### Analisis Bivariado


```{r}

par(mfrow = c(3, 3), xaxs = "i")
for (i in numeric_cols){
  df[['numeric_y']]=as.numeric(df[['y']])
  plot(x=df[[i]],
       y=df$numeric_y,
       xlab=i,
       ylab='Si=1 y No=0',
       main=i,
       col='blue')
}
```
Son muchos gráficos para juntarlos en uno, mejor sacarlo como ventana aparte y revisarlo uno a uno.

```{r}
categorias <- data.frame(combinations(length(unique(numeric_cols)),2,unique(numeric_cols)))
    for (i in 1:length(categorias$X1)){
      x=categorias[i,1]
      y=categorias[i,2]
      plot(df[[x]]
           ,df[[y]]
           ,col=df[['y']]
           ,xlab=x
           ,ylab=y,
           ,main=paste(c('Scatter ','Si=1 y No=0')))}
```


## KNN


### Normalizamos

```{r}
df_z <- df
df_z[all_of(numeric_cols)] <- df %>% 
  select(all_of(numeric_cols)) %>% 
  lapply(scale) %>% 
  as.data.frame()
```
```{r}
df$y %>% table/length(df$y)
```

### Split

```{r}
#Split
split <- initial_split(df_z, prop = 0.8, strata = "y")
df_train <- training(split) %>% select(-c('numeric_y'))
df_test <- testing(split) %>% select(-c('numeric_y'))

#Valida
round(prop.table(table(df_train$y)),2)
round(prop.table(table(df_test$y)),2)

#Asigna
y_train <- df_train$y
y_test <- df_test$y

# Borra y
```


### Entrenamiento

```{r}
# Especificar metodo de remuestreo
cv <- trainControl(method = "cv", number = 10)
#cv <- trainControl(method = "repeatedcv", number = 10, repeats = 5)

# Especificar parametros
#hyper_grid <- expand.grid(k = seq(1, 60, by = 2))

# Entrenamiento
#knn_fit <- train(y ~ ., data = df_train, method = "knn", 
                 #trControl = cv, tuneGrid = hyper_grid,  metric = "Accuracy")

#knn_fit
#ggplot(knn_fit)
```


### Prediccion
```{r}
knn_model <- knn3(y~.,df_train, k = 9)
knn_model_result <- predict(knn_model, newdata = df_test, type = 'class')
```

### Resultados

```{r}
### O
CrossTable(x = df_test$y, y = knn_model_result, prop.chisq = FALSE, 
           prop.c = FALSE, prop.r = FALSE)
```

```{r}
conf_matrix <- table(rep('no',length(df_test$y)), df_test$y)
conf_matrix
conf_matrix[1,1]/ length(as.numeric(knn_model_result))
```


```{r}
conf_matrix <- table(knn_model_result, df_test$y)
conf_matrix
(conf_matrix[1,1] + conf_matrix[2,2]) / length(as.numeric(knn_model_result))
```

## Decision Tree

Ocupamos los mismos datos que para el KNN, y también el CV.
Cambiamos los hiperparametros
```{r}
# Especificar parametros
hyper_grid_DT <- expand.grid(trials=c(50), model="tree", winnow = c(FALSE))

# Entrenamiento
DT_fit <- train(y ~ ., data = df_train, method = "C5.0", 
                 trControl = cv, tuneGrid = hyper_grid_DT,  metric = "Accuracy")

DT_fit
ggplot(DT_fit)
```


```{r}
DT_model <- C5.0(df_train %>% select(-'y'), df_train$y, trials = 100)
DT_result <- predict(DT_model, df_test %>%  select(-"y"))
```

```{r}
conf_matrix <- table(DT_result, df_test$y)
conf_matrix
(conf_matrix[1,1] + conf_matrix[2,2]) / length(as.numeric(DT_result))
```

Probando rpart

```{r}
DT_model2=rpart(y~.,data=df_train,method='class')
rpart.plot(DT_model2)
```


```{r}
DT_result2=predict(DT_model2,df_test,type='class')
```
```{r}
conf_matrix <- table(DT_result2, df_test$y)
conf_matrix
(conf_matrix[1,1] + conf_matrix[2,2]) / length(as.numeric(DT_result))
```




